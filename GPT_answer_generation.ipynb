{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kayblevision/Multilingual-ITS-using-GPT/blob/main/GPT_answer_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ti1DgnphI5Yc"
      },
      "source": [
        "# starting with accessing the API"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "begin with necessary import statements. Ensure you have your key (don't share it with anyone) in order to access the API."
      ],
      "metadata": {
        "id": "G9H_jhKjam81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade openai"
      ],
      "metadata": {
        "id": "OI8rTHGRnjDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7_U-o70T07Z"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"]='your api key goes here'\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "client = OpenAI()"
      ],
      "metadata": {
        "id": "Le5jNTpkaaiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing access\n",
        "response = client.embeddings.create(\n",
        "  model=\"text-embedding-ada-002\",\n",
        "  input=\"The food was delicious and the waiter...\"\n",
        ")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "id": "1slgYcV9nqkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThM_O5sIIFGi"
      },
      "source": [
        "# embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieve embeddings for a file you are working with. I'm using a .csv file for this project."
      ],
      "metadata": {
        "id": "Kp1iteBhbgHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai[embeddings]\n",
        "!pip install tiktoken\n",
        "!pip install utils"
      ],
      "metadata": {
        "id": "lVb5WOzUnux6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YlWQMCt8IKP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tiktoken\n",
        "import utils\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "specify which embedding model you want to use-- OpenAI has a few to choose from, linked [here](https://https://platform.openai.com/docs/guides/embeddings)"
      ],
      "metadata": {
        "id": "7KoKKOk6brjp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZF_R3X4ItWKQ"
      },
      "outputs": [],
      "source": [
        "def get_embedding(text, model=\"text-embedding-3-large\"):\n",
        "   text = text.replace(\"\\n\", \" \")\n",
        "   return client.embeddings.create(input = [text], model=model).data[0].embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GRV89P1jjVg"
      },
      "outputs": [],
      "source": [
        "embedding_model = \"text-embedding-3-large\"\n",
        "embedding_encoding = \"cl100k_base\"\n",
        "max_tokens = 8000  # the maximum for text-embedding-3-small is 8191. Choose the right number for you"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrP6w8gmh-x4"
      },
      "source": [
        "Loading the data (if using Google Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwHSD7ajh-nv",
        "outputId": "a53a602d-c7dc-4d10-c953-133297129b6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = '/path/to/data/goes/here/data.csv'\n",
        "df = pd.read_csv(data, header=None)"
      ],
      "metadata": {
        "id": "U_lPNQpNcqhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtain embeddings for your text data and save as in a new column in your dataframe."
      ],
      "metadata": {
        "id": "K1qPJkqbfq3N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9yKThppukkl"
      },
      "outputs": [],
      "source": [
        "import ast  # for converting embeddings saved as strings back to arrays\n",
        "from scipy import spatial  # for calculating vector similarities for search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6B10QNvOW51"
      },
      "outputs": [],
      "source": [
        "df['embedding'] = df['column_to_receive_embeddings'].apply(lambda x: get_embedding(x, model=embedding_model))\n",
        "df.to_csv('/file/save/path/name.csv', index=False) # save the file so you can access/load it later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LA3FYXOOWw2"
      },
      "outputs": [],
      "source": [
        "embed = pd.read_csv('/file/save/path/name.csv')\n",
        "embed.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cd9Hmh7TPbCv"
      },
      "outputs": [],
      "source": [
        "embed['embedding'] = embed['embedding'].apply(ast.literal_eval)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT modeling\n",
        "\n",
        "Define functions to introduce new data to the gpt model for question answering purposes. This includes a search function to rank text based on distance between embeddings, and a query message function to receive a user query, retrieve relevant texts and give a message to GPT.\n",
        "\n",
        "Reference code and helpful article found [here](https://cookbook.openai.com/examples/question_answering_using_embeddings)."
      ],
      "metadata": {
        "id": "LSIcIoMtgBEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_MODEL='gpt-3.5-turbo' # you can use this one or one of the newer gpt models"
      ],
      "metadata": {
        "id": "M042YTARgAYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nbu0M4O8Pura"
      },
      "outputs": [],
      "source": [
        "# search function for the dataset\n",
        "def strings_ranked_by_relatedness(query: str, df: pd.DataFrame,\n",
        "    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y),\n",
        "    top_n: int = 100\n",
        ") -> tuple[list[str], list[float]]:\n",
        "    \"\"\"Returns a list of strings and relatednesses, sorted from most related to least.\"\"\"\n",
        "    query_embedding_response = client.embeddings.create(\n",
        "        model=embedding_model,\n",
        "        input=query,\n",
        "    )\n",
        "    query_embedding = query_embedding_response.data[0].embedding\n",
        "    strings_and_relatednesses = [\n",
        "        (row[\"column_with_original_text\"], relatedness_fn(query_embedding, row[\"embedding\"]))\n",
        "        for i, row in embed.iterrows()\n",
        "    ]\n",
        "    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
        "    strings, relatednesses = zip(*strings_and_relatednesses)\n",
        "    return strings[:top_n], relatednesses[:top_n]\n",
        "\n",
        "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
        "    \"\"\"Return the number of tokens in a string.\"\"\"\n",
        "    encoding = tiktoken.encoding_for_model(model)\n",
        "    return len(encoding.encode(text))\n",
        "\n",
        "\n",
        "def query_message(\n",
        "    query: str,\n",
        "    df: pd.DataFrame = embed,\n",
        "    model: str = GPT_MODEL,\n",
        "    token_budget: int = 8000,\n",
        ") -> str:\n",
        "    \"\"\"Return a message for GPT.\"\"\"\n",
        "    strings, relatednesses = strings_ranked_by_relatedness(query, df2)\n",
        "    introduction = '''Use the information from the (text containing information) column to answer the subsequent question as succinctly\n",
        "    as possible. If the answer cannot be found in the data, write \"I could not find an answer.\"'''\n",
        "    question = f\"\\n\\nQuestion: {query}\"\n",
        "    message = introduction\n",
        "    return message + question"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now an ask function is defined to return GPT's answer. This code assumes input as a dataframe with relevant text, questions, and embeddings in the same row."
      ],
      "metadata": {
        "id": "VyaXCst-iDMc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JhNb1G1Pj8O"
      },
      "outputs": [],
      "source": [
        "def ask(\n",
        "    df: pd.DataFrame = df,\n",
        "    model: str = GPT_MODEL,\n",
        "    token_budget: int = 8000, # specify your own token budget\n",
        "    print_message: bool = False,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Answers questions based on (informational text) provided in the same row.\"\"\"\n",
        "    responses = []\n",
        "    for index, row in df.iterrows():\n",
        "        context = row['column_with_informational_text'] # replace with column names from your dataframe\n",
        "        question = row['question_column'] # replace with column names from your dataframe\n",
        "\n",
        "        context_message = query_message(context, model=model, token_budget=token_budget)\n",
        "        question_message = query_message(question, model=model, token_budget=token_budget)\n",
        "\n",
        "        if print_message:\n",
        "            print(\"Context message:\", context_message)\n",
        "            print(\"Question message:\", question_message)\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You answer questions about (relevant topic) based on the (relevant text).\"},\n",
        "            {\"role\": \"user\", \"content\": context_message},\n",
        "            {\"role\": \"user\", \"content\": question_message}\n",
        "        ]\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=messages,\n",
        "            temperature=0\n",
        "        )\n",
        "        response_message = response.choices[-1].message.content  # assuming the last message is the response to the question\n",
        "        responses.append(response_message)\n",
        "\n",
        "    df['response'] = responses\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a dataframe containing questions to be answered, this code gives output of the same dataframe, now with a 'response\n",
        "column with answers from GPT based on your provided data. However this function can also be adjusted to produce single answers, found at the reference code linked above.\n",
        "\n",
        "Access the dataframe with answers by calling the ask() function."
      ],
      "metadata": {
        "id": "Ah2vq0J6i-rR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zhxy79IIm__",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "answer = ask()\n",
        "answer.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyA7ysEtcMUM"
      },
      "outputs": [],
      "source": [
        "answer.to_csv('/file/path/here/filename.csv') # save the new dataframe to access it later"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Q-kxzvdIn5wZGOqrl6fqtrTOok3jyS71",
      "authorship_tag": "ABX9TyODfGowH1U5vQB1AvYd/mVl",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}